{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10385,"databundleVersionId":298493,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-14T16:21:47.875546Z","iopub.execute_input":"2024-07-14T16:21:47.876042Z","iopub.status.idle":"2024-07-14T16:21:48.885662Z","shell.execute_reply.started":"2024-07-14T16:21:47.875994Z","shell.execute_reply":"2024-07-14T16:21:48.884751Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/santander-customer-transaction-prediction/sample_submission.csv\n/kaggle/input/santander-customer-transaction-prediction/train.csv\n/kaggle/input/santander-customer-transaction-prediction/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"pd.set_option('max_columns', 200)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/santander-customer-transaction-prediction/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:21:51.517618Z","iopub.execute_input":"2024-07-14T16:21:51.518108Z","iopub.status.idle":"2024-07-14T16:22:11.036572Z","shell.execute_reply.started":"2024-07-14T16:21:51.518076Z","shell.execute_reply":"2024-07-14T16:22:11.035800Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:22:17.945956Z","iopub.execute_input":"2024-07-14T16:22:17.946319Z","iopub.status.idle":"2024-07-14T16:22:17.987712Z","shell.execute_reply.started":"2024-07-14T16:22:17.946290Z","shell.execute_reply":"2024-07-14T16:22:17.986841Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"   ID_code  target    var_0   var_1    var_2   var_3    var_4   var_5   var_6  \\\n0  train_0       0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187   \n1  train_1       0  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208   \n2  train_2       0   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427   \n3  train_3       0  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428   \n4  train_4       0   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405   \n\n     var_7  ...  var_190  var_191  var_192  var_193  var_194  var_195  \\\n0  18.6266  ...   4.4354   3.9642   3.1364   1.6910  18.5227  -2.3978   \n1  16.5338  ...   7.6421   7.7214   2.5837  10.9516  15.4305   2.0339   \n2  14.6155  ...   2.9057   9.7905   1.6704   1.6858  21.6042   3.1417   \n3  14.9250  ...   4.4666   4.7433   0.7178   1.4214  23.0347  -1.2706   \n4  19.2514  ...  -1.4905   9.5214  -0.1508   9.1942  13.2876  -1.5121   \n\n   var_196  var_197  var_198  var_199  \n0   7.8784   8.5635  12.7803  -1.0914  \n1   8.1267   8.7889  18.3560   1.9518  \n2  -6.5213   8.2675  14.7222   0.3965  \n3  -2.9275  10.2922  17.9697  -8.9996  \n4   3.9267   9.5031  17.9974  -8.8104  \n\n[5 rows x 202 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n      <th>target</th>\n      <th>var_0</th>\n      <th>var_1</th>\n      <th>var_2</th>\n      <th>var_3</th>\n      <th>var_4</th>\n      <th>var_5</th>\n      <th>var_6</th>\n      <th>var_7</th>\n      <th>...</th>\n      <th>var_190</th>\n      <th>var_191</th>\n      <th>var_192</th>\n      <th>var_193</th>\n      <th>var_194</th>\n      <th>var_195</th>\n      <th>var_196</th>\n      <th>var_197</th>\n      <th>var_198</th>\n      <th>var_199</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n      <td>0</td>\n      <td>8.9255</td>\n      <td>-6.7863</td>\n      <td>11.9081</td>\n      <td>5.0930</td>\n      <td>11.4607</td>\n      <td>-9.2834</td>\n      <td>5.1187</td>\n      <td>18.6266</td>\n      <td>...</td>\n      <td>4.4354</td>\n      <td>3.9642</td>\n      <td>3.1364</td>\n      <td>1.6910</td>\n      <td>18.5227</td>\n      <td>-2.3978</td>\n      <td>7.8784</td>\n      <td>8.5635</td>\n      <td>12.7803</td>\n      <td>-1.0914</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n      <td>0</td>\n      <td>11.5006</td>\n      <td>-4.1473</td>\n      <td>13.8588</td>\n      <td>5.3890</td>\n      <td>12.3622</td>\n      <td>7.0433</td>\n      <td>5.6208</td>\n      <td>16.5338</td>\n      <td>...</td>\n      <td>7.6421</td>\n      <td>7.7214</td>\n      <td>2.5837</td>\n      <td>10.9516</td>\n      <td>15.4305</td>\n      <td>2.0339</td>\n      <td>8.1267</td>\n      <td>8.7889</td>\n      <td>18.3560</td>\n      <td>1.9518</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n      <td>0</td>\n      <td>8.6093</td>\n      <td>-2.7457</td>\n      <td>12.0805</td>\n      <td>7.8928</td>\n      <td>10.5825</td>\n      <td>-9.0837</td>\n      <td>6.9427</td>\n      <td>14.6155</td>\n      <td>...</td>\n      <td>2.9057</td>\n      <td>9.7905</td>\n      <td>1.6704</td>\n      <td>1.6858</td>\n      <td>21.6042</td>\n      <td>3.1417</td>\n      <td>-6.5213</td>\n      <td>8.2675</td>\n      <td>14.7222</td>\n      <td>0.3965</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n      <td>0</td>\n      <td>11.0604</td>\n      <td>-2.1518</td>\n      <td>8.9522</td>\n      <td>7.1957</td>\n      <td>12.5846</td>\n      <td>-1.8361</td>\n      <td>5.8428</td>\n      <td>14.9250</td>\n      <td>...</td>\n      <td>4.4666</td>\n      <td>4.7433</td>\n      <td>0.7178</td>\n      <td>1.4214</td>\n      <td>23.0347</td>\n      <td>-1.2706</td>\n      <td>-2.9275</td>\n      <td>10.2922</td>\n      <td>17.9697</td>\n      <td>-8.9996</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n      <td>0</td>\n      <td>9.8369</td>\n      <td>-1.4834</td>\n      <td>12.8746</td>\n      <td>6.6375</td>\n      <td>12.2772</td>\n      <td>2.4486</td>\n      <td>5.9405</td>\n      <td>19.2514</td>\n      <td>...</td>\n      <td>-1.4905</td>\n      <td>9.5214</td>\n      <td>-0.1508</td>\n      <td>9.1942</td>\n      <td>13.2876</td>\n      <td>-1.5121</td>\n      <td>3.9267</td>\n      <td>9.5031</td>\n      <td>17.9974</td>\n      <td>-8.8104</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 202 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:22:18.447504Z","iopub.execute_input":"2024-07-14T16:22:18.447828Z","iopub.status.idle":"2024-07-14T16:22:18.525983Z","shell.execute_reply.started":"2024-07-14T16:22:18.447789Z","shell.execute_reply":"2024-07-14T16:22:18.525118Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"ID_code    0\ntarget     0\nvar_0      0\nvar_1      0\nvar_2      0\n          ..\nvar_195    0\nvar_196    0\nvar_197    0\nvar_198    0\nvar_199    0\nLength: 202, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"cols = train_df.columns.tolist()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:22:18.527673Z","iopub.execute_input":"2024-07-14T16:22:18.527966Z","iopub.status.idle":"2024-07-14T16:22:18.531976Z","shell.execute_reply.started":"2024-07-14T16:22:18.527941Z","shell.execute_reply":"2024-07-14T16:22:18.531164Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"colsobject_columns = train_df.select_dtypes(include=['object'])","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:22:18.533415Z","iopub.execute_input":"2024-07-14T16:22:18.533915Z","iopub.status.idle":"2024-07-14T16:22:18.542625Z","shell.execute_reply.started":"2024-07-14T16:22:18.533881Z","shell.execute_reply":"2024-07-14T16:22:18.541833Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"colsobject_columns","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:22:18.544289Z","iopub.execute_input":"2024-07-14T16:22:18.544546Z","iopub.status.idle":"2024-07-14T16:22:18.560278Z","shell.execute_reply.started":"2024-07-14T16:22:18.544523Z","shell.execute_reply":"2024-07-14T16:22:18.559462Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"             ID_code\n0            train_0\n1            train_1\n2            train_2\n3            train_3\n4            train_4\n...              ...\n199995  train_199995\n199996  train_199996\n199997  train_199997\n199998  train_199998\n199999  train_199999\n\n[200000 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID_code</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>199995</th>\n      <td>train_199995</td>\n    </tr>\n    <tr>\n      <th>199996</th>\n      <td>train_199996</td>\n    </tr>\n    <tr>\n      <th>199997</th>\n      <td>train_199997</td>\n    </tr>\n    <tr>\n      <th>199998</th>\n      <td>train_199998</td>\n    </tr>\n    <tr>\n      <th>199999</th>\n      <td>train_199999</td>\n    </tr>\n  </tbody>\n</table>\n<p>200000 rows Ã— 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train_df.set_index(\"ID_code\", inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:22:19.552616Z","iopub.execute_input":"2024-07-14T16:22:19.553425Z","iopub.status.idle":"2024-07-14T16:22:19.558379Z","shell.execute_reply.started":"2024-07-14T16:22:19.553394Z","shell.execute_reply":"2024-07-14T16:22:19.557361Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X = train_df.drop(['target'], axis = 1)\ny = train_df['target']","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:22:20.265618Z","iopub.execute_input":"2024-07-14T16:22:20.265941Z","iopub.status.idle":"2024-07-14T16:22:20.361376Z","shell.execute_reply.started":"2024-07-14T16:22:20.265915Z","shell.execute_reply":"2024-07-14T16:22:20.360607Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:22:20.935035Z","iopub.execute_input":"2024-07-14T16:22:20.935329Z","iopub.status.idle":"2024-07-14T16:22:21.845458Z","shell.execute_reply.started":"2024-07-14T16:22:20.935306Z","shell.execute_reply":"2024-07-14T16:22:21.844642Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\ny_pred_dt = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:22:22.001126Z","iopub.execute_input":"2024-07-14T16:22:22.001888Z","iopub.status.idle":"2024-07-14T16:25:29.668268Z","shell.execute_reply.started":"2024-07-14T16:22:22.001846Z","shell.execute_reply":"2024-07-14T16:25:29.667174Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:38:15.321205Z","iopub.execute_input":"2024-07-14T16:38:15.321560Z","iopub.status.idle":"2024-07-14T16:50:57.888633Z","shell.execute_reply.started":"2024-07-14T16:38:15.321531Z","shell.execute_reply":"2024-07-14T16:50:57.887811Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\ndt_cm = confusion_matrix(y_test, y_pred_dt)\nrf_cm = confusion_matrix(y_test, y_pred_rf)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:50:57.890594Z","iopub.execute_input":"2024-07-14T16:50:57.890975Z","iopub.status.idle":"2024-07-14T16:50:57.933461Z","shell.execute_reply.started":"2024-07-14T16:50:57.890947Z","shell.execute_reply":"2024-07-14T16:50:57.932713Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"dt_cm\ndisp = ConfusionMatrixDisplay(confusion_matrix=dt_cm, display_labels=train_df.target)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:56:56.685105Z","iopub.execute_input":"2024-07-14T16:56:56.685457Z","iopub.status.idle":"2024-07-14T16:56:56.689918Z","shell.execute_reply.started":"2024-07-14T16:56:56.685423Z","shell.execute_reply":"2024-07-14T16:56:56.688998Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(disp)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:57:24.783852Z","iopub.execute_input":"2024-07-14T16:57:24.784204Z","iopub.status.idle":"2024-07-14T16:57:27.076068Z","shell.execute_reply.started":"2024-07-14T16:57:24.784177Z","shell.execute_reply":"2024-07-14T16:57:27.074549Z"},"trusted":true},"execution_count":19,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_axes.py:1690\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1688\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[0;32m-> 1690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m scalex:\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request_autoscale_view(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:2304\u001b[0m, in \u001b[0;36m_AxesBase.add_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   2301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2302\u001b[0m     line\u001b[38;5;241m.\u001b[39mset_clip_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch)\n\u001b[0;32m-> 2304\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_line_limits\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line\u001b[38;5;241m.\u001b[39mget_label():\n\u001b[1;32m   2306\u001b[0m     line\u001b[38;5;241m.\u001b[39mset_label(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_child\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_children)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/axes/_base.py:2327\u001b[0m, in \u001b[0;36m_AxesBase._update_line_limits\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m   2323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_line_limits\u001b[39m(\u001b[38;5;28mself\u001b[39m, line):\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2325\u001b[0m \u001b[38;5;124;03m    Figures out the data limit of the given line, updating self.dataLim.\u001b[39;00m\n\u001b[1;32m   2326\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2327\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[43mline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path\u001b[38;5;241m.\u001b[39mvertices\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   2329\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/lines.py:1028\u001b[0m, in \u001b[0;36mLine2D.get_path\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return the `~matplotlib.path.Path` associated with this line.\"\"\"\u001b[39;00m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidx:\n\u001b[0;32m-> 1028\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/lines.py:664\u001b[0m, in \u001b[0;36mLine2D.recache\u001b[0;34m(self, always)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m always \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invalidy:\n\u001b[1;32m    663\u001b[0m     yconv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_yunits(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_yorig)\n\u001b[0;32m--> 664\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43m_to_unmasked_float_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43myconv\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    666\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_y\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1345\u001b[0m, in \u001b[0;36m_to_unmasked_float_array\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39masarray(x, \u001b[38;5;28mfloat\u001b[39m)\u001b[38;5;241m.\u001b[39mfilled(np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m)\u001b[49m\n","\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'ConfusionMatrixDisplay'"],"ename":"TypeError","evalue":"float() argument must be a string or a real number, not 'ConfusionMatrixDisplay'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"rf_cm","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:55:54.140042Z","iopub.execute_input":"2024-07-14T16:55:54.140826Z","iopub.status.idle":"2024-07-14T16:55:54.147357Z","shell.execute_reply.started":"2024-07-14T16:55:54.140781Z","shell.execute_reply":"2024-07-14T16:55:54.146078Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"array([[44880,     0],\n       [ 5118,     2]])"},"metadata":{}}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.special import logit\nimport lightgbm as lgb\n\n# Assuming train_df and test_df are already defined and loaded\n\nfeatures = [x for x in train_df.columns if x.startswith(\"var\")]\nhist_df = pd.DataFrame()\n\nfor var in features:\n    var_stats = pd.concat([train_df[var], test_df[var]]).value_counts()\n    hist_df[var] = pd.Series(test_df[var]).map(var_stats)\n    hist_df[var] = hist_df[var] > 1\n\nind = hist_df.sum(axis=1) != len(features)  # Changed 200 to len(features)\nvar_stats = {var: pd.concat([train_df[var], test_df[ind][var]]).value_counts() for var in features}\n\npred = 0\nfor var in features:\n    model = lgb.LGBMClassifier(**{\n        'learning_rate': 0.05, 'max_bin': 165, 'max_depth': 5, 'min_child_samples': 150,\n        'min_child_weight': 0.1, 'min_split_gain': 0.0018, 'n_estimators': 41,\n        'num_leaves': 6, 'reg_alpha': 2.0, 'reg_lambda': 2.54, 'objective': 'binary', 'n_jobs': -1\n    })\n    \n    X_train = np.hstack([train_df[var].values.reshape(-1, 1),\n                         train_df[var].map(var_stats[var]).values.reshape(-1, 1)])\n    y_train = train_df[\"target\"].values\n    \n    model.fit(X_train, y_train)\n    \n    X_test = np.hstack([test_df[var].values.reshape(-1, 1),\n                        test_df[var].map(var_stats[var]).values.reshape(-1, 1)])\n    \n    pred += logit(model.predict_proba(X_test)[:, 1])\n\n# Create submission file\nsubmission = pd.DataFrame({\"ID_code\": test_df[\"ID_code\"], \"target\": pred})\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T16:58:09.945195Z","iopub.execute_input":"2024-07-14T16:58:09.945544Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3493: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n  if await self.run_code(code, result, async_=asy):\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003202 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 180\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001501 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001520 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 182\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001620 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 183\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001343 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 185\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001520 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001343 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 196\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001620 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001533 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 180\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001451 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 188\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001529 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001516 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001380 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 264\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001629 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001342 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 182\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001408 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 212\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001670 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 181\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001552 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001564 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001594 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 175\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001579 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001556 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001548 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 180\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001342 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 205\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001543 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001338 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 225\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001527 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001444 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 186\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001374 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 197\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 180\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001547 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 175\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001608 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 186\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001359 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 182\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001524 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005041 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 204\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001532 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001554 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 180\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001335 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 183\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001530 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001525 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001601 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001673 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001420 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 197\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001515 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 220\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001532 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001500 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 173\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001675 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 181\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001507 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 175\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015313 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 175\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001626 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001501 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 199\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001540 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001488 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001391 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 198\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001552 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 175\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001533 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001544 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 181\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001353 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 195\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001510 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001400 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 194\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001530 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001521 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 173\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001356 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 184\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001506 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001361 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 187\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002015 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001507 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 189\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001548 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001343 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 321\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001655 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001601 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 175\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001399 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 228\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001614 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001528 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 174\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001511 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001562 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001650 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 184\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001542 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 187\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001534 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001430 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 184\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001577 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 175\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001525 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001487 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007032 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001492 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001379 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 182\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005931 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 180\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001548 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 175\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001363 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 260\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001558 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001373 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 208\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001381 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 182\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001549 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 201\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001564 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 174\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001506 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 174\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001370 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 200\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001486 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 185\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001833 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 174\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001544 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001538 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 175\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001384 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 244\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001388 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 184\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001346 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 196\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.078527 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 184\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001467 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 278\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001482 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001506 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 180\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001651 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 191\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001421 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 186\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001520 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001481 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 191\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001352 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 183\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001382 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 185\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001595 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 174\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001518 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 176\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001641 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001490 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 174\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001417 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 185\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 178\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001576 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001575 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 180\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001508 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 221\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001426 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 219\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001523 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 180\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001524 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 179\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001533 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 177\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001359 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 196\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n[LightGBM] [Info] Number of positive: 20098, number of negative: 179902\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001349 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 208\n[LightGBM] [Info] Number of data points in the train set: 200000, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.100490 -> initscore=-2.191792\n[LightGBM] [Info] Start training from score -2.191792\n","output_type":"stream"}]}]}